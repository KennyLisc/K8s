# 部署kube-controller-manager

## 创建kube-controller-manager证书和密钥

```bash
[root@vm01 ~]# mkdir ssl/controller-manager
[root@vm01 ~]# cd ssl/controller-manager/
[root@vm01 controller-manager]# vi kube-controller-manager-csr.json
{
  "CN": "system:kube-controller-manager",
    "hosts": [
      "127.0.0.1",
      "172.16.16.81",
      "172.16.16.82"
      ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
  "C": "CN",
  "L": "Fujian",
  "ST": "Fuzhou",
  "O": "system:kube-controller-manager",
  "OU": "System"
    }
  ]
}
```

* kube-controller-manager服务只运行在vm01和vm02；

```bash
[root@vm01 controller-manager]# cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
    -ca-key=/etc/kubernetes/ssl/ca-key.pem \
    -config=/etc/kubernetes/ssl/ca-config.json \
    -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare \
    kube-controller-manager

2018/06/28 11:21:33 [INFO] generate received request
2018/06/28 11:21:33 [INFO] received CSR
2018/06/28 11:21:33 [INFO] generating key: rsa-2048
2018/06/28 11:21:34 [INFO] encoded CSR
2018/06/28 11:21:34 [INFO] signed certificate with serial number 368682508998549683227036438065201993598215411701
2018/06/28 11:21:34 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").
```

```bash
[root@vm01 controller-manager]# cp kube-controller-manager*.pem /etc/kubernetes/ssl/
[root@vm01 controller-manager]# scp kube-controller-manager*.pem root@vm02:/etc/kubernetes/ssl/
```

* 分发kube-controller-manager证书和密钥文件到vm02；

## 创建kubeconfig文件

```bash
[root@vm01 controller-manager]# kubectl config set-cluster kubernetes \
    --certificate-authority=/etc/kubernetes/ssl/ca.pem \
    --embed-certs=true \
    --server=https://172.16.16.200:6443 \
    --kubeconfig=kube-controller-manager.kubeconfig
Cluster "kubernetes" set.

[root@vm01 controller-manager]# kubectl config set-credentials kube-controller-manager \
    --client-certificate=/etc/kubernetes/ssl/kube-controller-manager.pem \
    --client-key=/etc/kubernetes/ssl/kube-controller-manager-key.pem \
    --embed-certs=true \
    --kubeconfig=kube-controller-manager.kubeconfig
User "kube-controller-manager" set.

[root@vm01 controller-manager]#  kubectl config set-context default \
    --cluster=kubernetes \
    --user=kube-controller-manager \
    --kubeconfig=kube-controller-manager.kubeconfig
Context "default" created.

[root@vm01 controller-manager]# kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig
Switched to context "default".

[root@vm01 controller-manager]# kubectl config view --kubeconfig=kube-controller-manager.kubeconfig
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://172.16.16.200:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kube-controller-manager
  name: default
current-context: default
kind: Config
preferences: {}
users:
- name: kube-controller-manager
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED

[root@vm01 controller-manager]# cp kube-controller-manager.kubeconfig /etc/kubernetes/
[root@vm01 controller-manager]# scp kube-controller-manager.kubeconfig root@vm02:/etc/kubernetes/
```

* 将kubeconfig文件分发到vm02；

## 配置kube-controller-manager运行参数

```bash
[root@vm01 ~]# vi controller-manager
KUBE_CONTROLLER_MANAGER_ARGS="--bind-address=172.16.16.81 \
    --port=0 \
    --secure-port=10252 \
    --allocate-node-cidrs=true \
    --cluster-name=kubernetes \
    --service-cluster-ip-range=10.254.0.0/16 \
    --cluster-cidr=10.253.0.0/16 \
    --controllers=*,bootstrapsigner,tokencleaner \
    --use-service-account-credentials=true \
    --cluster-name=kubernetes \
    --experimental-cluster-signing-duration=8760h \
    --feature-gates=RotateKubeletServerCertificate=true \
    --horizontal-pod-autoscaler-use-rest-clients=true \
    --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \
    --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \
    --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \
    --tls-cert-file=/etc/kubernetes/ssl/kube-controller-manager.pem \
    --tls-private-key-file=/etc/kubernetes/ssl/kube-controller-manager-key.pem \
    --root-ca-file=/etc/kubernetes/ssl/ca.pem \
    --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \
    --leader-elect=true"
[root@vm01 ~]# cp controller-manager /etc/kubernetes/
[root@vm01 ~]# scp controller-manager root@vm02:/etc/kubernetes/
[root@vm02 ~]# vi /etc/kubernetes/controller-manager
KUBE_CONTROLLER_MANAGER_ARGS="--bind-address=172.16.16.82 \
```

* `--port=0`关闭监听http/metrics的请求；
* `--secure-port`、`--bind-address`在172.16.16.81监听10252端口的https/metrics请求；
* `--kubeconfig`指定kubeconfig文件路径，kube-controller-manager使用它连接和验证kube-apiserver；
* `--cluster-signing-*-file`签名TLS Bootstrap创建的证书；
* `--experimental-cluster-signing-duration`指定TLS Bootstrap证书的有效期；
* `--root-ca-file`放置到容器ServiceAccount中的CA证书，用来对kube-apiserver的证书进行校验；
* `--service-account-private-key-file`签名ServiceAccount中Token的私钥文件，必须和kube-apiserver的`--service-account-key-file`指定的公钥文件配对使用；
* `--service-cluster-ip-range`指定Service ClusterIP网段，必须和kube-apiserver中的同名参数一致；
* `--leader-elect=true`集群运行模式，启用选举功能,被选为leader的节点负责处理工作，其它节点为阻塞状态；
* `--feature-gates=RotateKubeletServerCertificate=true`开启kubletserver证书的自动更新特性；
* `--controllers=*,bootstrapsigner,tokencleaner`启用的控制器列表，tokencleaner用于自动清理过期的Bootstrap token；
* `--horizontal-pod-autoscaler-*`custom metrics相关参数，支持autoscaling/v2alpha1；
* `--tls-cert-file`、`--tls-private-key-file`使用https输出metrics时使用的Server证书和秘钥；

## 配置kube-controller-manager服务

```bash
[root@vm01 ~]# vi kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
After=kube-apiserver.service

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/controller-manager
ExecStart=/usr/bin/kube-controller-manager \
    $KUBE_LOGTOSTDERR \
    $KUBE_LOG_LEVEL \
    $KUBE_CONTROLLER_MANAGER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
[root@vm01 ~]# cp kube-controller-manager.service /usr/lib/systemd/system/
[root@vm01 ~]# scp kube-controller-manager.service root@vm02:/usr/lib/systemd/system
```

## 启动kube-controller-manager服务

```bash
[root@vm01 ~]# systemctl daemon-reload
[root@vm01 ~]# systemctl enable kube-controller-manager
Created symlink from /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service to /usr/lib/systemd/system/kube-controller-manager.service.
[root@vm01 ~]# systemctl start kube-controller-manager.service
[root@vm02 ~]# systemctl daemon-reload
[root@vm02 ~]# systemctl enable kube-controller-manager
Created symlink from /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service to /usr/lib/systemd/system/kube-controller-manager.service.
[root@vm02 ~]# systemctl start kube-controller-manager
```

* 通过journalctl -t kube-controller-manager检查服务启动情况；

## 测试kube-controller-manager metrics

```bash
[root@vm01 ~]# curl -s --cacert /etc/kubernetes/ssl/ca.pem https://127.0.0.1:10252/metrics |head
# HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.
# TYPE apiserver_audit_event_total counter
apiserver_audit_event_total 0
# HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.
# TYPE apiserver_client_certificate_expiration_seconds histogram
apiserver_client_certificate_expiration_seconds_bucket{le="0"} 0
apiserver_client_certificate_expiration_seconds_bucket{le="21600"} 0
apiserver_client_certificate_expiration_seconds_bucket{le="43200"} 0
apiserver_client_certificate_expiration_seconds_bucket{le="86400"} 0
apiserver_client_certificate_expiration_seconds_bucket{le="172800"} 0

[root@vm02 ~]# curl -s --cacert /etc/kubernetes/ssl/ca.pem https://127.0.0.1:10252/metrics |head
# HELP ClusterRoleAggregator_adds Total number of adds handled by workqueue: ClusterRoleAggregator
# TYPE ClusterRoleAggregator_adds counter
ClusterRoleAggregator_adds 3
# HELP ClusterRoleAggregator_depth Current depth of workqueue: ClusterRoleAggregator
# TYPE ClusterRoleAggregator_depth gauge
ClusterRoleAggregator_depth 0
# HELP ClusterRoleAggregator_queue_latency How long an item stays in workqueueClusterRoleAggregator before being requested.
# TYPE ClusterRoleAggregator_queue_latency summary
ClusterRoleAggregator_queue_latency{quantile="0.5"} 57544
ClusterRoleAggregator_queue_latency{quantile="0.9"} 58047
```

## 检看kube-controller-manager选举信息

```bash
[root@vm01 ~]# kubectl get endpoints kube-controller-manager --namespace=kube-system  -o yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"vm02_25d5e0c4-7a84-11e8-87d7-005056a3453f","leaseDurationSeconds":15,"acquireTime":"2018-06-28T03:34:38Z","renewTime":"2018-06-28T03:49:51Z","leaderTransitions":43}'
  creationTimestamp: 2018-06-15T15:41:03Z
  name: kube-controller-manager
  namespace: kube-system
  resourceVersion: "1310254"
  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager
  uid: 82c9c10b-70b2-11e8-becb-005056a37751
```