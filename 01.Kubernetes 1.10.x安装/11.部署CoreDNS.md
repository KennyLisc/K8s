# 部署CoreDNS

## 安装CoreDNS

```bash
[root@vm01 ~]# wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed
[root@vm01 ~]# mv coredns.yaml.sed coredns.yaml
[root@vm01 ~]# vi coredns.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  - services
  - pods
  - namespaces
  verbs:
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
- kind: ServiceAccount
  name: coredns
  namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local 10.254.0.0/16 {
          pods insecure
          upstream
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        proxy . /etc/resolv.conf
        cache 30
        reload
    }
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/name: "CoreDNS"
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
    spec:
      serviceAccountName: coredns
      tolerations:
        - key: "CriticalAddonsOnly"
          operator: "Exists"
      containers:
      - name: coredns
        image: coredns/coredns:1.1.3
        imagePullPolicy: IfNotPresent
        args: [ "-conf", "/etc/coredns/Corefile" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
          readOnly: true
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
      dnsPolicy: Default
      volumes:
        - name: config-volume
          configMap:
            name: coredns
            items:
            - key: Corefile
              path: Corefile
---
apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  annotations:
    prometheus.io/scrape: "true"
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: "CoreDNS"
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: 10.254.0.2
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
```

* configMap定义部分将`kubernetes CLUSTER_DOMAIN REVERSE_CIDRS`改为集群DNS域和服务CIDR，这里设置为：`kubernetes cluster.local 10.254.0.0/16`
* service定义部分，将`clusterIP: CLUSTER_DNS_IP`改为kube-dns的service地址`10.254.0.2`；

```bash
[root@vm01 ~]# kubectl create -f coredns.yaml
serviceaccount "coredns" created
clusterrole.rbac.authorization.k8s.io "system:coredns" created
clusterrolebinding.rbac.authorization.k8s.io "system:coredns" created
configmap "coredns" created
deployment.extensions "coredns" created
service "kube-dns" created
```

## 测试CoreDNS解析

```bash
[root@vm01 ~]# vi nginx-dnstest.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-dnstest
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx-dns
    spec:
      containers:
      - name: nginx-dnstest
        image: nginx
        ports:
        - containerPort: 80
[root@vm01 ~]# kubectl create -f nginx-dnstest.yaml
deployment.extensions "nginx-dnstest" created
```

```bash
[root@vm01 ~]# vi nginx-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.7.9
    ports:
    - containerPort: 80
[root@vm01 ~]# kubectl create -f nginx-pod.yaml
pod "nginx" created
[root@vm01 ~]# kubectl expose deploy nginx-dnstest
service "nginx-dnstest" exposed
```

```bash
[root@vm01 ~]# kubectl get all
NAME                                READY     STATUS    RESTARTS   AGE
pod/nginx                           1/1       Running   1          1h
pod/nginx-dnstest-99d4fd74d-vrfvs   1/1       Running   1          1h
pod/nginx-dnstest-99d4fd74d-wgsbc   1/1       Running   1          1h

NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubernetes      ClusterIP   10.254.0.1      <none>        443/TCP   3d
service/nginx-dnstest   ClusterIP   10.254.184.23   <none>        80/TCP    23s

NAME                            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nginx-dnstest   2         2         2            2           1h

NAME                                      DESIRED   CURRENT   READY     AGE
replicaset.apps/nginx-dnstest-99d4fd74d   2         2         2         1h
```

```bash
root@nginx:/# ping nginx-dnstest
PING nginx-dnstest.default.svc.cluster.local (10.254.184.23): 48 data bytes

root@nginx:/# ping nginx-dnstest.default.svc.cluster.local
PING nginx-dnstest.default.svc.cluster.local (10.254.184.23): 48 data bytes

root@nginx:/# ping kube-dns.kube-system.svc.cluster.local
PING kube-dns.kube-system.svc.cluster.local (10.254.0.2): 48 data bytes
36 bytes from 218.207.153.130: Destination Net Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  04 4c00 f60a   0 0040  01  01 b835 10.253.35.3  10.254.0.2
^C--- kube-dns.kube-system.svc.cluster.local ping statistics ---
1 packets transmitted, 0 packets received, 100% packet loss

root@nginx:/# ping kubernetes
PING kubernetes.default.svc.cluster.local (10.254.0.1): 48 data bytes
36 bytes from 218.207.153.130: Destination Net Unreachable
Vr HL TOS  Len   ID Flg  off TTL Pro  cks      Src      Dst Data
 4  5  04 4c00 859a   0 0040  01  01 29a6 10.253.35.3  10.254.0.1
^C--- kubernetes.default.svc.cluster.local ping statistics ---
1 packets transmitted, 0 packets received, 100% packet loss
```

* 测试时只需看ping记录中是否能将域名解析为ip。如果kube-proxy开启了ipvs模式，则可以ping服务ip，若使用iptables，不能ping通服务ip。