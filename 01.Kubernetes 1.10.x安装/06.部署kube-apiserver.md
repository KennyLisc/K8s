# 部署kube-apiserver

kube-apiserver本身是一个无状态服务，通过keepalived vrrp和haproxy的tcp转发，很容易实现高可用和负载均衡，为其他服务提供统一入口。

## 创建kubernetes证书和密钥

```bash
[root@vm01 ~]# mkdir ssl/kubernetes
[root@vm01 ~]# cd ssl/kubernetes/
[root@vm01 kubernetes]# vi kubernetes-csr.json
{
  "CN": "kubernetes",
  "hosts": [
    "127.0.0.1",
    "172.16.16.81",
    "172.16.16.82",
    "172.16.16.200",
    "10.254.0.1",
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
        "C": "CN",
        "L": "Fujian",
        "ST": "Fuzhou",
        "O": "k8s",
        "OU": "System"
    }
  ]
}
```

* hosts字段指定授权使用该证书的IP和域名列表，172.16.16.200为apiserver的LVS VIP；
* kube-apiserver所使用的的证书要包含VIP地址，否则其他服务无法正常访问kube-apiserver VIP；

```bash
[root@vm01 kubernetes]# cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
    -ca-key=/etc/kubernetes/ssl/ca-key.pem \
    -config=/etc/kubernetes/ssl/ca-config.json \
    -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes

2018/06/14 10:14:40 [INFO] generate received request
2018/06/14 10:14:40 [INFO] received CSR
2018/06/14 10:14:40 [INFO] generating key: rsa-2048
2018/06/14 10:14:40 [INFO] encoded CSR
2018/06/14 10:14:40 [INFO] signed certificate with serial number 668244337077888569214278318567909909028486481278
2018/06/14 10:14:40 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").
```

```bash
[root@vm01 kubernetes]# cp kubernetes*.pem /etc/kubernetes/ssl/
[root@vm01 kubernetes]# scp kubernetes*.pem root@vm02:/etc/kubernetes/ssl/
```

* 将kubernetes证书和密钥分发到Master节点，这里为vm01、vm02；

## 配置kube-apiserver审计策略

```bash
[root@vm01 ~]# cat >> audit-policy.yaml <<EOF
# Log all requests at the Metadata level.
apiVersion: audit.k8s.io/v1beta1
kind: Policy
rules:
- level: Metadata
EOF

[root@vm01 ~]# cp audit-policy.yaml /etc/kubernetes/
[root@vm01 ~]# scp audit-policy.yaml root@vm02:/etc/kubernetes/
```

* 记录metadata级别请求；
* 分发审计策略文件到vm01和vm02节点；

## 配置kube-apiserver运行参数

```bash
[root@vm01 ~]# vi config
KUBE_LOGTOSTDERR="--logtostderr=true"

KUBE_LOG_LEVEL="--v=0"

KUBE_ALLOW_PRIV="--allow-privileged=true"

KUBE_MASTER_SECURE="--master=https://172.16.16.200:6443"
[root@vm01 ~]# cp config /etc/kubernetes/
[root@vm01 ~]# scp config root@vm02:/etc/kubernetes/
[root@vm01 ~]# scp config root@vm03:/etc/kubernetes/
[root@vm01 ~]# scp config root@vm04:/etc/kubernetes/
```

* `--logtostderr=true`输出到标准错误，会将日志记录到journald，后续可通过journalctl查看日志；
* `--allow-privileged=true`允许运行特权容器；
* `--master=https://172.16.16.200:6443`将master指向kube-apiserver的VIP；
* 将config配置文件分发到所有节点；

```bash
[root@vm01 ~]# vi apiserver
KUBE_API_ADDRESS="--advertise-address=172.16.16.81 --bind-address=172.16.16.81 --insecure-bind-address=172.16.16.81"

KUBE_ETCD_SERVERS="--etcd-servers=https://172.16.16.81:2379,https://172.16.16.82:2379,https://172.16.16.83:2379"

KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=10.254.0.0/16"

KUBE_ADMISSION_CONTROL="--enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota"

KUBE_API_ARGS="--authorization-mode=Node,RBAC \
    --insecure-port=0 \
    --kubelet-https=true \
    --anonymous-auth=false \
    --enable-bootstrap-token-auth \
    --service-node-port-range=30000-32767 \
    --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \
    --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
    --client-ca-file=/etc/kubernetes/ssl/ca.pem \
    --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \
    --storage-backend=etcd3 \
    --etcd-cafile=/etc/kubernetes/ssl/ca.pem \
    --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \
    --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \
    --kubelet-client-certificate=/etc/kubernetes/ssl/kubernetes.pem \
    --kubelet-client-key=/etc/kubernetes/ssl/kubernetes-key.pem \
    --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \
    --enable-swagger-ui=true \
    --audit-policy-file=/etc/kubernetes/audit-policy.yaml \
    --apiserver-count=2 \
    --audit-log-maxage=30 \
    --audit-log-maxbackup=3 \
    --audit-log-maxsize=100 \
    --audit-log-path=/var/log/k8s-audit.log \
    --event-ttl=1h"
```

* `--advertise-address`向集群成员通知apiserver消息的IP地址；
* `--bind-address`和`--insecure-bind-address`设置kube-apiserver监听地址，不要设置为127.0.0.1,否则其他节点无法访问kube-apiserver。也不要设置为0.0.0.0，设置为0.0.0.0可能导致后续起kube-proxy ipvs模式情况下系统服务端口和应用service ipvs监听端口冲突；
* `--service-cluster-ip-range`指定应用service的ip段；
* `--enable-admission-plugins`控制资源进入集群的准入控制插件的顺序列表；
* `--authorization-mode=Node,RBAC`开启Node和RBAC授权模式，拒绝未授权的请求；
* `--insecure-port=0`关闭监听8080非安全端口；
* `--kubelet-https=true`为kubelet启用https；
* `--enable-bootstrap-token-auth`启用kubelet bootstrap的token认证；
* `--service-node-port-range`设置nodeport的端口范围；
* `--tls-*-file`指定apiserver使用的证书、密钥和CA文件；
* `--client-ca-file`用于验证client(`kue-controller-manager、kube-scheduler、kubelet、kube-proxy`等)请求所带的证书；
* `--etcd-*file`指定用于访问连接etcd服务的CA、证书和密钥文件；
* `--kubelet-client`指定用于访问kubelet的的证书和密钥文件；
* `--service-account-key-file`签名ServiceAccount Token的公钥文件，kube-controller-manager的`--service-account-private-key-file`指定私钥文件，两者配对使用；
* `--audit-policy-file`定义审计策略配置的文件的路径；
* `--audit-log-*`配置审计日志的相关参数；

```bash
[root@vm01 ~]# cp apiserver /etc/kubernetes/
[root@vm01 ~]# scp apiserver root@vm02:/etc/kubernetes/
```

* 将apiserver配置文件分发到vm02节点；

```bash
[root@vm02 ~]# vi /etc/kubernetes/apiserver
KUBE_API_ADDRESS="--advertise-address=172.16.16.82 --bind-address=172.16.16.82 --insecure-bind-address=172.16.16.82"
```

* 修改vm02 apiserver相关监听地址；

## 配置kube-apiserver服务

```bash
[root@vm01 ~]# vi kube-apiserver.service
[Unit]
Description=Kubernetes API Service
After=network.target
After=etcd.service

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/apiserver
ExecStart=/usr/bin/kube-apiserver \
    $KUBE_LOGTOSTDERR \
    $KUBE_LOG_LEVEL \
    $KUBE_ETCD_SERVERS \
    $KUBE_API_ADDRESS \
    $KUBE_ALLOW_PRIV \
    $KUBE_SERVICE_ADDRESSES \
    $KUBE_ADMISSION_CONTROL \
    $KUBE_API_ARGS
Restart=on-failure
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
[root@vm01 ~]# scp kube-apiserver.service root@vm02:/usr/lib/systemd/system/
```

## 启动kube-apiserver

```bash
[root@vm01 ~]# systemctl daemon-reload
[root@vm01 ~]# systemctl enable kube-apiserver.service
[root@vm01 ~]# systemctl start kube-apiserver.service
```

```bash
[root@vm02 ~]# systemctl daemon-reload
[root@vm02 ~]# systemctl enable kube-apiserver.service
[root@vm02 ~]# systemctl start kube-apiserver.service
```

## 授权kubernetes用户访问kubelet

```bash
[root@vm01 ~]# kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes
clusterrolebinding.rbac.authorization.k8s.io "kube-apiserver:kubelet-apis" created
```

* 在执行kubectl exec、run、logs等命令时，apiserver会转发到kubelet。这里定义RBAC规则，授权apiserver调用kubelet API。

## 安装配置keepalived

```bash
[root@vm01 ~]# yum install keepalived
[root@vm02 ~]# yum install keepalived
```

* 在两个master（vm01、vm02）节点安装keepalived服务；

```bash
[root@vm01 ~]# vi /etc/keepalived/keepalived.conf
global_defs {
   router_id k8s
}

vrrp_instance kube_api {
    state BACKUP
    interface ens160
    virtual_router_id 80
    nopreempt
    priority 120
    advert_int 3
    authentication {
        auth_type PASS
        auth_pass b1946a
    }
    virtual_ipaddress {
       172.16.16.200/24 dev ens160 scope global
    }
    track_script {
        haproxy-check weight 20
    }
}
vrrp_script haproxy-check {
    script "killall -0 haproxy"
    interval 5
    weight 20
}
[root@vm02 ~]# vi /etc/keepalived/keepalived.conf
    priority 60
    #nopreempt
```

* 创建vrrp，vip为172.16.16.200/24；

```bash
[root@vm01 ~]# systemctl daemon-reload
[root@vm01 ~]# systemctl enable keepalived
[root@vm01 ~]# systemctl start keepalived
[root@vm02 ~]# systemctl daemon-reload
[root@vm02 ~]# systemctl enable keepalived
[root@vm02 ~]# systemctl start keepalived
```

* 运行keepalived服务；

## 安装配置haproxy

```bash
# yum install haproxy -y
```

```bash
# vi /etc/sysctl.conf
net.ipv4.ip_nonlocal_bind = 1
# sysctl -p
```

* 开启`net.ipv4.ip_nonlocal_bind`，即使vip没在当前机器也能将haproxy监听在vip上；

```bash
[root@vm01 ~]# vi /etc/haproxy/haproxy.cfg
global
    log         127.0.0.1 local2
    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon
    stats socket /var/lib/haproxy/stats

defaults
    log                     global
    option                  dontlognull
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout check           10s
    maxconn                 3000

#frontend  kube_api_insecure
#    bind 172.16.16.200:8080
#    mode tcp
#    option tcplog
#    default_backend kube_api_insecure
#
#backend kube_api_insecure
#    mode tcp
#    option tcplog
#    option tcp-check
#    balance roundrobin
#    stick match src
#    stick-table type ip size 200k expire 30m
#    server  vm01 172.16.16.81:8080 check inter 5s rise 4 fall 3
#    server  vm02 172.16.16.82:8080 check inter 5s rise 4 fall 3

frontend  kube_api_secure
    bind  172.16.16.200:6443
    mode tcp
    option tcplog
    default_backend kube_api_secure

backend kube_api_secure
    mode tcp
    option tcplog
    option tcp-check
    balance roundrobin
    stick match src
    stick-table type ip size 200k expire 30m
    server  vm01 172.16.16.81:6443 check inter 5s rise 4 fall 3
    server  vm02 172.16.16.82:6443 check inter 5s rise 4 fall 3

[root@vm01 ~]# scp /etc/haproxy/haproxy.cfg root@vm02:/etc/haproxy/
```

* 设置haproxy服务，并开启基于ip的session保持；
* 将访问172.16.16.200:6443的请求转发至172.16.16.81:6443和172.16.16.82:6443；

```bash
[root@vm01 ~]# systemctl enable haproxy.service
Created symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service.
[root@vm01 ~]# systemctl start haproxy

[root@vm02 ~]# systemctl enable haproxy.service
Created symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service.
[root@vm02 ~]# systemctl start haproxy
```

* 启动haproxy服务；

## 修改kubeconfig文件

```bash
[root@vm01 ~]# vi .kube/config
    server: https://172.16.16.200:6443
```

* 修改admin用户的kubeconfig文件，将ip指向kube-apiserver的vip。